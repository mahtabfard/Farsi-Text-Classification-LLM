{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8f9aSwWA8xR"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMXPz7V7A_i1",
        "outputId": "77840370-e378-408e-a8fc-c6be6d24a208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate peft sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syC6JjFTWuAx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiNJt3DZBLYc"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqJiUgReBO2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d2eb08-337f-44e7-bb34-9a1d2619a153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_p9BI-lFz3H"
      },
      "source": [
        "# Testing The Model Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klr36-d4HE6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2978f13d-7a78-412b-9c6c-5d6d65dba022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\",\n",
            "    \"1\": \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\",\n",
            "    \"2\": \"\\u0641\\u0631\\u0647\\u0646\\u06af\",\n",
            "    \"3\": \"\\u0648\\u0631\\u0632\\u0634\\u06cc\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\": 0,\n",
            "    \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\": 1,\n",
            "    \"\\u0641\\u0631\\u0647\\u0646\\u06af\": 2,\n",
            "    \"\\u0648\\u0631\\u0632\\u0634\\u06cc\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--HooshvareLab--bert-fa-base-uncased/snapshots/a04aa40c97bcdde570ae11986a534542c2995a62/pytorch_model.bin\n",
            "Attempting to create safetensors variant\n",
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Enter Ø®Ø§Ù„ÛŒ Ø¨Ø²Ù†ÛŒØ¯):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Safetensors PR exists\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ù…ØªÙ†:  Ø¯Ø± Ù¾ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¯Ø± Ù‡ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø§ Ø§ØªØ®Ø§Ø° Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ØªÙ„Ø§Ø´ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø«Ø¨Ø§Øª Ù†Ø³Ø¨ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ Ø¹Ø±Ø¶Ù‡ Ø§Ø±Ø² Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ù†ÛŒÙ…Ø§ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ùˆ Ø³Ù‚Ù Ø®Ø±ÛŒØ¯ Ø§Ø±Ø² Ø¨Ø±Ø§ÛŒ ÙˆØ§Ø±Ø¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø§Ø¶Ø§ÛŒ Ø§Ø±Ø² Ù…Ø³Ø§ÙØ±ØªÛŒ Ùˆ ØªØ¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø³Øª Ø§Ø¬Ø±Ø§Ø³Øª. Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ù…Ø¹ØªÙ‚Ø¯Ù†Ø¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø±Ø§Ù‡Ú©Ø§Ø± Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø§Ø± ØªÙˆØ±Ù… Ùˆ Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø§Ø±Ø²ÛŒØŒ ØªÙ‚ÙˆÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¬Ø°Ø¨ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØµØ§Ø¯Ø±Ø§Øª ØºÛŒØ±Ù†ÙØªÛŒ Ø§Ø³Øª. Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø®Ø´ Ø®ØµÙˆØµÛŒ Ù†ÛŒØ² Ø®ÙˆØ§Ø³ØªØ§Ø± Ú©Ø§Ù‡Ø´ Ù…ÙˆØ§Ù†Ø¹ ØªØ¬Ø§Ø±ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ù†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ù‡ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
            "Ù…ØªÙ†:  Ø¯Ø± Ù…Ø±Ø§Ø³Ù… Ø§ÙØªØªØ§Ø­ÛŒÙ‡ Ø³ÛŒâ€ŒÙˆÙ†Ù‡Ù…ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ ÙÛŒÙ„Ù… ÙØ¬Ø±ØŒ Ú©Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ú¯Ø³ØªØ±Ø¯Ù‡ Ø§Ù‡Ø§Ù„ÛŒ Ø³ÛŒÙ†Ù…Ø§ØŒ Ù…Ù†ØªÙ‚Ø¯Ø§Ù† Ùˆ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¢Ø«Ø§Ø± Ø´Ø§Ø®ØµÛŒ Ø§Ø² Ø³ÛŒÙ†Ù…Ø§Ú¯Ø±Ø§Ù† Ø¬ÙˆØ§Ù† Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø¢Ù…Ø¯. Ø§Ù…Ø³Ø§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø§ÙˆØ±ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ù…Ø³Ø§Ø¨Ù‚Ù‡ØŒ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ú˜Ø§Ù†Ø±Ù‡Ø§ Ùˆ Ø³Ø¨Ú©â€ŒÙ‡Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ù…Ø³Ø§Ø¦Ù„ Ø±ÙˆØ² Ø¬Ø§Ù…Ø¹Ù‡ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨ÛŒØ´ØªØ± Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø­Ø§Ø´ÛŒÙ‡ Ø§ÛŒÙ† Ù…Ø±Ø§Ø³Ù…ØŒ Ø§Ø² Ú†Ù†Ø¯ Ù‡Ù†Ø±Ù…Ù†Ø¯ Ù¾ÛŒØ´Ú©Ø³ÙˆØª Ù†ÛŒØ² ØªÙ‚Ø¯ÛŒØ± Ø¨Ù‡â€ŒØ¹Ù…Ù„ Ø¢Ù…Ø¯ Ùˆ Ù†Ø´Ø³ØªÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Â«Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ù‡ÙˆÛŒØª ÙØ±Ù‡Ù†Ú¯ÛŒÂ» Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯. Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ù‡Ø¯Ù Ø§Ù…Ø³Ø§Ù„ØŒ ØªÙ‚ÙˆÛŒØª ØªØ¹Ø§Ù…Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø³ÛŒÙ†Ù…Ø§Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø­Ù…Ø§ÛŒØª Ø§Ø² ØªÙˆÙ„ÛŒØ¯Ø§Øª Ù…Ø³ØªÙ‚Ù„ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø³Øª.\n",
            "Ù…ØªÙ†:  ØªÛŒÙ… Ù…Ù„ÛŒ ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ú†Ù‡Ø§Ø±Ù…ÛŒÙ† Ø¯ÛŒØ¯Ø§Ø± Ø®ÙˆØ¯ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ú¯Ø±ÙˆÙ‡ÛŒ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯ Ù…Ù„Øªâ€ŒÙ‡Ø§ÛŒ Û²Û°Û²Ûµ Ø¨Ù‡ Ù…ØµØ§Ù ØªÛŒÙ… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø±ÙØª. Ø§ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„Ù† Ø§Ù„Ù…Ù¾ÛŒÚ© ØªÙˆÚ©ÛŒÙˆ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù¾Ø±Ø´ÙˆØ± ØªÙ…Ø§Ø´Ø§Ú¯Ø±Ø§Ù† Ùˆ Ù¾Ø®Ø´ Ø²Ù†Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø´Ø¨Ú©Ù‡ ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨ÙˆØ¯. ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ùˆ Ø¯ÙØ§Ø¹ Ù…Ù†Ø³Ø¬Ù…ØŒ Ù…ÙˆÙÙ‚ Ø´Ø¯Ù†Ø¯ Ø³Øª Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ Ø¨Ù‡â€ŒÙ†ÙØ¹ Ø®ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ø³Ø§Ù†Ù†Ø¯. Ù‡Ø±Ú†Ù†Ø¯ ØªÛŒÙ… Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø¯Ø± Ø³Øª Ø¯ÙˆÙ… Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¯Ø§Ø´ØªØŒ Ø§Ù…Ø§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ø¯Ø±Ø®Ø´Ø´ Ø³Ø¹ÛŒØ¯ Ù…Ø¹Ø±ÙˆÙ Ùˆ Ø§Ù…ÛŒØ± ØºÙÙˆØ±ØŒ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø¯Ø± Ø³Øª Ù¾Ù†Ø¬Ù… Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ Û³ Ø¨Ø± Û² Ø¨Ù‡ Ø³ÙˆØ¯ Ø®ÙˆØ¯ ØªÙ…Ø§Ù… Ú©Ø±Ø¯. Ø³Ø±Ù…Ø±Ø¨ÛŒ ØªÛŒÙ… Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø§Ø² ØªØ¹Ù‡Ø¯ Ùˆ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†Ø´ ØªÙ…Ø¬ÛŒØ¯ Ú©Ø±Ø¯ Ùˆ ÙˆØ¹Ø¯Ù‡ Ø¯Ø§Ø¯ ØªÛŒÙ… Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ø­Ø§Ø¶Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
            "Ù…ØªÙ†:  Ø¯Ø± Ø¢Ø³ØªØ§Ù†Ù‡ ÙØµÙ„ ØªØ§Ø¨Ø³ØªØ§Ù†ØŒ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡â€ŒÚ©Ø§Ø± Ú©Ø±Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø³ØªØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø´ÙˆØ± Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ ØºØ±ÙÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø¬Ø§Ø°Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒØŒ Ø¢ÛŒÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒØŒ ØºØ°Ø§Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯Ø§Øª ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ø±Ø¯Ù†Ø¯. ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ù†ÙˆØ§Ø­ÛŒØŒ Ú©Ø§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø³ÙØ§Ù„â€ŒÚ¯Ø±ÛŒØŒ ÙØ±Ø´â€ŒØ¨Ø§ÙÛŒ Ùˆ Ø³Ø§Ø®Øª Ø²ÛŒÙˆØ±Ø¢Ù„Ø§Øª Ø³Ù†ØªÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ ØªÙˆØ§Ù†Ø³Øª ØªÙˆØ¬Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ø¬Ù„Ø¨ Ú©Ù†Ø¯. Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ ØªØ±ÙˆÛŒØ¬ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø´Ø§ØºÙ„ Ø¨ÙˆÙ…ÛŒ Ùˆ ØªÙ‚ÙˆÛŒØª Ø§Ù‚ØªØµØ§Ø¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ± ØªØ®ÙÛŒÙÛŒ Ù†ÛŒØ² ØªÙˆØ³Ø· Ø¢Ú˜Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø³Ø§ÙØ±ØªÛŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.\n",
            "Ù…ØªÙ†: \n",
            "\n",
            "Ù†ØªØ§ÛŒØ¬ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„ Ø®Ø§Ù…:\n",
            "Ù…ØªÙ†: Ø¯Ø± Ù¾ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¯Ø± Ù‡ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø§ Ø§ØªØ®Ø§Ø° Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ØªÙ„Ø§Ø´ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø«Ø¨Ø§Øª Ù†Ø³Ø¨ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ Ø¹Ø±Ø¶Ù‡ Ø§Ø±Ø² Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ù†ÛŒÙ…Ø§ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ùˆ Ø³Ù‚Ù Ø®Ø±ÛŒØ¯ Ø§Ø±Ø² Ø¨Ø±Ø§ÛŒ ÙˆØ§Ø±Ø¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø§Ø¶Ø§ÛŒ Ø§Ø±Ø² Ù…Ø³Ø§ÙØ±ØªÛŒ Ùˆ ØªØ¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø³Øª Ø§Ø¬Ø±Ø§Ø³Øª. Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ù…Ø¹ØªÙ‚Ø¯Ù†Ø¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø±Ø§Ù‡Ú©Ø§Ø± Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø§Ø± ØªÙˆØ±Ù… Ùˆ Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø§Ø±Ø²ÛŒØŒ ØªÙ‚ÙˆÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¬Ø°Ø¨ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØµØ§Ø¯Ø±Ø§Øª ØºÛŒØ±Ù†ÙØªÛŒ Ø§Ø³Øª. Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø®Ø´ Ø®ØµÙˆØµÛŒ Ù†ÛŒØ² Ø®ÙˆØ§Ø³ØªØ§Ø± Ú©Ø§Ù‡Ø´ Ù…ÙˆØ§Ù†Ø¹ ØªØ¬Ø§Ø±ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ù†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ù‡ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
            "Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø· (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: 35.58%)\n",
            "\n",
            "Ù…ØªÙ†: Ø¯Ø± Ù…Ø±Ø§Ø³Ù… Ø§ÙØªØªØ§Ø­ÛŒÙ‡ Ø³ÛŒâ€ŒÙˆÙ†Ù‡Ù…ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ ÙÛŒÙ„Ù… ÙØ¬Ø±ØŒ Ú©Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ú¯Ø³ØªØ±Ø¯Ù‡ Ø§Ù‡Ø§Ù„ÛŒ Ø³ÛŒÙ†Ù…Ø§ØŒ Ù…Ù†ØªÙ‚Ø¯Ø§Ù† Ùˆ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¢Ø«Ø§Ø± Ø´Ø§Ø®ØµÛŒ Ø§Ø² Ø³ÛŒÙ†Ù…Ø§Ú¯Ø±Ø§Ù† Ø¬ÙˆØ§Ù† Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø¢Ù…Ø¯. Ø§Ù…Ø³Ø§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø§ÙˆØ±ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ù…Ø³Ø§Ø¨Ù‚Ù‡ØŒ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ú˜Ø§Ù†Ø±Ù‡Ø§ Ùˆ Ø³Ø¨Ú©â€ŒÙ‡Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ù…Ø³Ø§Ø¦Ù„ Ø±ÙˆØ² Ø¬Ø§Ù…Ø¹Ù‡ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨ÛŒØ´ØªØ± Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø­Ø§Ø´ÛŒÙ‡ Ø§ÛŒÙ† Ù…Ø±Ø§Ø³Ù…ØŒ Ø§Ø² Ú†Ù†Ø¯ Ù‡Ù†Ø±Ù…Ù†Ø¯ Ù¾ÛŒØ´Ú©Ø³ÙˆØª Ù†ÛŒØ² ØªÙ‚Ø¯ÛŒØ± Ø¨Ù‡â€ŒØ¹Ù…Ù„ Ø¢Ù…Ø¯ Ùˆ Ù†Ø´Ø³ØªÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Â«Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ù‡ÙˆÛŒØª ÙØ±Ù‡Ù†Ú¯ÛŒÂ» Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯. Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ù‡Ø¯Ù Ø§Ù…Ø³Ø§Ù„ØŒ ØªÙ‚ÙˆÛŒØª ØªØ¹Ø§Ù…Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø³ÛŒÙ†Ù…Ø§Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø­Ù…Ø§ÛŒØª Ø§Ø² ØªÙˆÙ„ÛŒØ¯Ø§Øª Ù…Ø³ØªÙ‚Ù„ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø³Øª.\n",
            "Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø· (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: 37.31%)\n",
            "\n",
            "Ù…ØªÙ†: ØªÛŒÙ… Ù…Ù„ÛŒ ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ú†Ù‡Ø§Ø±Ù…ÛŒÙ† Ø¯ÛŒØ¯Ø§Ø± Ø®ÙˆØ¯ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ú¯Ø±ÙˆÙ‡ÛŒ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯ Ù…Ù„Øªâ€ŒÙ‡Ø§ÛŒ Û²Û°Û²Ûµ Ø¨Ù‡ Ù…ØµØ§Ù ØªÛŒÙ… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø±ÙØª. Ø§ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„Ù† Ø§Ù„Ù…Ù¾ÛŒÚ© ØªÙˆÚ©ÛŒÙˆ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù¾Ø±Ø´ÙˆØ± ØªÙ…Ø§Ø´Ø§Ú¯Ø±Ø§Ù† Ùˆ Ù¾Ø®Ø´ Ø²Ù†Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø´Ø¨Ú©Ù‡ ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨ÙˆØ¯. ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ùˆ Ø¯ÙØ§Ø¹ Ù…Ù†Ø³Ø¬Ù…ØŒ Ù…ÙˆÙÙ‚ Ø´Ø¯Ù†Ø¯ Ø³Øª Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ Ø¨Ù‡â€ŒÙ†ÙØ¹ Ø®ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ø³Ø§Ù†Ù†Ø¯. Ù‡Ø±Ú†Ù†Ø¯ ØªÛŒÙ… Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø¯Ø± Ø³Øª Ø¯ÙˆÙ… Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¯Ø§Ø´ØªØŒ Ø§Ù…Ø§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ø¯Ø±Ø®Ø´Ø´ Ø³Ø¹ÛŒØ¯ Ù…Ø¹Ø±ÙˆÙ Ùˆ Ø§Ù…ÛŒØ± ØºÙÙˆØ±ØŒ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø¯Ø± Ø³Øª Ù¾Ù†Ø¬Ù… Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ Û³ Ø¨Ø± Û² Ø¨Ù‡ Ø³ÙˆØ¯ Ø®ÙˆØ¯ ØªÙ…Ø§Ù… Ú©Ø±Ø¯. Ø³Ø±Ù…Ø±Ø¨ÛŒ ØªÛŒÙ… Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø§Ø² ØªØ¹Ù‡Ø¯ Ùˆ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†Ø´ ØªÙ…Ø¬ÛŒØ¯ Ú©Ø±Ø¯ Ùˆ ÙˆØ¹Ø¯Ù‡ Ø¯Ø§Ø¯ ØªÛŒÙ… Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ø­Ø§Ø¶Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
            "Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø· (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: 42.43%)\n",
            "\n",
            "Ù…ØªÙ†: Ø¯Ø± Ø¢Ø³ØªØ§Ù†Ù‡ ÙØµÙ„ ØªØ§Ø¨Ø³ØªØ§Ù†ØŒ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡â€ŒÚ©Ø§Ø± Ú©Ø±Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø³ØªØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø´ÙˆØ± Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ ØºØ±ÙÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø¬Ø§Ø°Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒØŒ Ø¢ÛŒÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒØŒ ØºØ°Ø§Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯Ø§Øª ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ø±Ø¯Ù†Ø¯. ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ù†ÙˆØ§Ø­ÛŒØŒ Ú©Ø§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø³ÙØ§Ù„â€ŒÚ¯Ø±ÛŒØŒ ÙØ±Ø´â€ŒØ¨Ø§ÙÛŒ Ùˆ Ø³Ø§Ø®Øª Ø²ÛŒÙˆØ±Ø¢Ù„Ø§Øª Ø³Ù†ØªÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ ØªÙˆØ§Ù†Ø³Øª ØªÙˆØ¬Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ø¬Ù„Ø¨ Ú©Ù†Ø¯. Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ ØªØ±ÙˆÛŒØ¬ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø´Ø§ØºÙ„ Ø¨ÙˆÙ…ÛŒ Ùˆ ØªÙ‚ÙˆÛŒØª Ø§Ù‚ØªØµØ§Ø¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ± ØªØ®ÙÛŒÙÛŒ Ù†ÛŒØ² ØªÙˆØ³Ø· Ø¢Ú˜Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø³Ø§ÙØ±ØªÛŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.\n",
            "Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø· (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: 37.15%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ØºÛŒØ±ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ W&B\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ (ÙØ§Ø±Ø³ÛŒ)\n",
        "labels_list = ['Ø§Ù‚ØªØµØ§Ø¯ÛŒ', 'ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·', 'ÙØ±Ù‡Ù†Ú¯', 'ÙˆØ±Ø²Ø´ÛŒ']\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø®Ø§Ù… Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±\n",
        "MODEL_NAME = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(labels_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Ø¯Ø±ÛŒØ§ÙØª ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø±\n",
        "print(\"Ù„Ø·ÙØ§Ù‹ ÛŒÚ© ÛŒØ§ Ú†Ù†Ø¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† Enter Ø®Ø§Ù„ÛŒ Ø¨Ø²Ù†ÛŒØ¯):\")\n",
        "texts = []\n",
        "while True:\n",
        "    line = input(\"Ù…ØªÙ†: \").strip()\n",
        "    if line == \"\":\n",
        "        break\n",
        "    texts.append(line)\n",
        "\n",
        "if not texts:\n",
        "    print(\"Ù…ØªÙ†ÛŒ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯Ù‡.\")\n",
        "    exit()\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´\n",
        "inputs = tokenizer(\n",
        "    texts,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬\n",
        "print(\"\\nÙ†ØªØ§ÛŒØ¬ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„ Ø®Ø§Ù…:\")\n",
        "for i, text in enumerate(texts):\n",
        "    label = id2label[preds[i].item()]\n",
        "    confidence = probs[i][preds[i]].item()\n",
        "    print(f\"Ù…ØªÙ†: {text}\")\n",
        "    print(f\"Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: {label} (Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm4n7RP5WYTZ"
      },
      "source": [
        "# LORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPMRIPXxYxr7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ====\n",
        "MODEL_NAME = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Final/lora_train.json\"\n",
        "VAL_FILE = \"/content/drive/MyDrive/Final/lora_val.json\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Final/final-bert-fa-lora\"\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ==== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ====\n",
        "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "with open(VAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "labels_list = sorted(list(set(item['label'] for item in train_data + val_data)))\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "NUM_LABELS = len(labels_list)\n",
        "\n",
        "# ==== Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ù†Ù…ÙˆØ¯Ø§Ø± ====\n",
        "# ØªØ±ØªÛŒØ¨ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ labels_list ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§Ø´Ø¯\n",
        "label_translation = {\n",
        "    'Ø§Ù‚ØªØµØ§Ø¯ÛŒ': 'Economic',\n",
        "    'ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·': 'Entertainment',\n",
        "    'ÙØ±Ù‡Ù†Ú¯': 'Culture',\n",
        "    'ÙˆØ±Ø²Ø´ÛŒ': 'Sports'\n",
        "}\n",
        "labels_list_en = [label_translation[label] for label in labels_list]\n",
        "\n",
        "# ==== ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ====\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    text = examples[\"text\"]\n",
        "    labels = [label2id[label] for label in examples[\"label\"]]\n",
        "    encodings = tokenizer(\n",
        "        text,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    encodings[\"labels\"] = labels\n",
        "    return encodings\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
        "val_dataset = Dataset.from_list(val_data).map(preprocess_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns([\"text\", \"label\"])\n",
        "val_dataset = val_dataset.remove_columns([\"text\", \"label\"])\n",
        "\n",
        "# ==== Ù…Ø¯Ù„ ====\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# ==== ØªØ§Ø¨Ø¹ Ù…ØªØ±ÛŒÚ© ====\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ==== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´ ====\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    log_level=\"info\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    do_eval=True,\n",
        "    load_best_model_at_end=False,\n",
        "    disable_tqdm=False,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# ==== Trainer ====\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# ==== Ø¢Ù…ÙˆØ²Ø´ ====\n",
        "print(\"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\")\n",
        "trainer.train()\n",
        "\n",
        "# ==== Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ ====\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "trainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\n",
        "print(\"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\")\n",
        "\n",
        "# ==== Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ====\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "print(\"\\nğŸ“Š Classification Report:\")\n",
        "print(classification_report(labels, preds, target_names=labels_list))\n",
        "\n",
        "# ==== Confusion Matrix ====\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    xticklabels=labels_list_en,\n",
        "    yticklabels=labels_list_en,\n",
        "    cmap=\"Blues\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray'\n",
        ")\n",
        "plt.xlabel(\"Predicted\", fontsize=12)\n",
        "plt.ylabel(\"True\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==== Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ JSON ====\n",
        "results_data = {\n",
        "    \"train_loss\": float(trainer.state.log_history[-1][\"train_loss\"]) if \"train_loss\" in trainer.state.log_history[-1] else 0,\n",
        "    \"val_accuracy\": float(accuracy_score(labels, preds)),\n",
        "    \"classification_report\": classification_report(labels, preds, target_names=labels_list, output_dict=True),\n",
        "    \"confusion_matrix\": cm.tolist(),\n",
        "    \"labels\": labels_list\n",
        "}\n",
        "\n",
        "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
        "with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"âœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ù…Ø³ÛŒØ± Ø²ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {results_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# ==== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ====\n",
        "MODEL_NAME = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Final/lora_train.json\"\n",
        "VAL_FILE = \"/content/drive/MyDrive/Final/lora_val.json\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Final1/final-bert-fa-lora\"\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ==== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ====\n",
        "with open(TRAIN_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "with open(VAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "labels_list = sorted(list(set(item['label'] for item in train_data + val_data)))\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "NUM_LABELS = len(labels_list)\n",
        "\n",
        "label_translation = {\n",
        "    'Ø§Ù‚ØªØµØ§Ø¯ÛŒ': 'Economic',\n",
        "    'ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·': 'Entertainment',\n",
        "    'ÙØ±Ù‡Ù†Ú¯': 'Culture',\n",
        "    'ÙˆØ±Ø²Ø´ÛŒ': 'Sports'\n",
        "}\n",
        "labels_list_en = [label_translation[label] for label in labels_list]\n",
        "\n",
        "# ==== ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ====\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    text = examples[\"text\"]\n",
        "    labels = [label2id[label] for label in examples[\"label\"]]\n",
        "    encodings = tokenizer(\n",
        "        text,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    encodings[\"labels\"] = labels\n",
        "    return encodings\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
        "val_dataset = Dataset.from_list(val_data).map(preprocess_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns([\"text\", \"label\"])\n",
        "val_dataset = val_dataset.remove_columns([\"text\", \"label\"])\n",
        "\n",
        "# ==== Ù…Ø¯Ù„ Ø¨Ø§ Dropout Ø¨ÛŒØ´ØªØ± ====\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    hidden_dropout_prob=0.3,     # â† Ø§ÙØ²Ø§ÛŒØ´ Dropout\n",
        "    attention_probs_dropout_prob=0.3  # â† Dropout Ø¯Ø± ØªÙˆØ¬Ù‡\n",
        ")\n",
        "\n",
        "# ==== ØªØ§Ø¨Ø¹ Ù…ØªØ±ÛŒÚ© ====\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ==== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´ ====\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,  # â† Ú©Ù…Ú© Ø¨Ù‡ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",  # â† Ø¨Ø±Ø§ÛŒ EarlyStopping Ù„Ø§Ø²Ù… Ø§Ø³Øª\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    do_eval=True,\n",
        "    load_best_model_at_end=True,  # â† Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    log_level=\"info\"\n",
        ")\n",
        "\n",
        "# ==== Trainer Ø¨Ø§ EarlyStopping ====\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # â† ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù…\n",
        ")\n",
        "\n",
        "# ==== Ø¢Ù…ÙˆØ²Ø´ ====\n",
        "print(\"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\")\n",
        "trainer.train()\n",
        "\n",
        "# ==== Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ ====\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "trainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\n",
        "print(\"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\")\n",
        "\n",
        "# ==== Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ====\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "print(\"\\nğŸ“Š Classification Report:\")\n",
        "print(classification_report(labels, preds, target_names=labels_list))\n",
        "\n",
        "# ==== Confusion Matrix ====\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    xticklabels=labels_list_en,\n",
        "    yticklabels=labels_list_en,\n",
        "    cmap=\"Blues\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray'\n",
        ")\n",
        "plt.xlabel(\"Predicted\", fontsize=12)\n",
        "plt.ylabel(\"True\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "# ==== Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± JSON ====\n",
        "results_data = {\n",
        "    \"train_loss\": float(trainer.state.log_history[-1].get(\"train_loss\", 0)),\n",
        "    \"val_accuracy\": float(accuracy_score(labels, preds)),\n",
        "    \"classification_report\": classification_report(labels, preds, target_names=labels_list, output_dict=True),\n",
        "    \"confusion_matrix\": cm.tolist(),\n",
        "    \"labels\": labels_list\n",
        "}\n",
        "\n",
        "results_path = os.path.join(OUTPUT_DIR, \"results.json\")\n",
        "with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"âœ… Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ù…Ø³ÛŒØ± Ø²ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {results_path}\")\n"
      ],
      "metadata": {
        "id": "2oeF7jXFL-xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeQYP4J_cz_3"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KENBKEIEWb2O"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ==== ØºÛŒØ±ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ W&B ====\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ==== Ù…Ø³ÛŒØ±Ù‡Ø§ ====\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Final/final-bert-fa-lora/best_model\"\n",
        "TEST_FILE = \"/content/drive/MyDrive/Final/lora_test.json\"\n",
        "\n",
        "# ==== Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ (ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ) ====\n",
        "labels_list = ['Ø§Ù‚ØªØµØ§Ø¯ÛŒ', 'ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·', 'ÙØ±Ù‡Ù†Ú¯', 'ÙˆØ±Ø²Ø´ÛŒ']\n",
        "label2id = {label: i for i, label in enumerate(labels_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# ==== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ====\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    local_files_only=True,\n",
        "    num_labels=len(labels_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# ==== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª ====\n",
        "with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "test_data_cleaned = [item for item in test_data if item.get(\"label\") in label2id]\n",
        "removed_count = len(test_data) - len(test_data_cleaned)\n",
        "print(f\"âš ï¸ {removed_count} Ù†Ù…ÙˆÙ†Ù‡ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯ Ø¨Ù‡â€ŒØ¯Ù„ÛŒÙ„ Ù„ÛŒØ¨Ù„ Ù†Ø§Ø¯Ø±Ø³Øª ÛŒØ§ Ø®Ø§Ù„ÛŒ.\")\n",
        "\n",
        "# ==== Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ====\n",
        "def preprocess_function(examples):\n",
        "    texts = examples[\"text\"]\n",
        "    labels = [label2id[label] for label in examples[\"label\"]]\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "    encodings[\"labels\"] = labels\n",
        "    return encodings\n",
        "\n",
        "test_dataset = Dataset.from_list(test_data_cleaned).map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.remove_columns([\"text\", \"label\"])\n",
        "\n",
        "# ==== Ù…ØªØ±ÛŒÚ© ====\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ==== Trainer ====\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ==== Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ====\n",
        "print(\"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª...\")\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "metrics = predictions.metrics\n",
        "\n",
        "# ==== Ù†ØªØ§ÛŒØ¬ ====\n",
        "print(\"\\nğŸ“‰ Test Loss:\", round(metrics[\"test_loss\"], 4))\n",
        "print(\"âœ… Test Accuracy:\", round(metrics[\"test_accuracy\"], 4))\n",
        "\n",
        "print(\"\\nğŸ“Š Classification Report:\")\n",
        "print(classification_report(labels, preds, target_names=labels_list))\n",
        "\n",
        "# ==== Confusion Matrix ====\n",
        "labels_list_en = ['Economic', 'Entertainment', 'Culture', 'Sports']\n",
        "\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    xticklabels=labels_list_en,\n",
        "    yticklabels=labels_list_en,\n",
        "    cmap=\"YlGnBu\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray'\n",
        ")\n",
        "plt.xlabel(\"Predicted Labels\", fontsize=12)\n",
        "plt.ylabel(\"True Labels\", fontsize=12)\n",
        "plt.title(\"Confusion Matrix - Test Set\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJFFF_AakTw"
      },
      "source": [
        "# Unseen Data(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh-8VjDUaxYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8994f0c2-d30d-41b0-cbab-9a75de1e9d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading file chat_template.jinja\n",
            "loading configuration file /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.3,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\",\n",
            "    \"1\": \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\",\n",
            "    \"2\": \"\\u0641\\u0631\\u0647\\u0646\\u06af\",\n",
            "    \"3\": \"\\u0648\\u0631\\u0632\\u0634\\u06cc\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\": 0,\n",
            "    \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\": 1,\n",
            "    \"\\u0641\\u0631\\u0647\\u0646\\u06af\": 2,\n",
            "    \"\\u0648\\u0631\\u0632\\u0634\\u06cc\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            ">  Ø¯Ø± Ù¾ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¯Ø± Ù‡ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø§ Ø§ØªØ®Ø§Ø° Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ØªÙ„Ø§Ø´ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø«Ø¨Ø§Øª Ù†Ø³Ø¨ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ Ø¹Ø±Ø¶Ù‡ Ø§Ø±Ø² Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ù†ÛŒÙ…Ø§ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ùˆ Ø³Ù‚Ù Ø®Ø±ÛŒØ¯ Ø§Ø±Ø² Ø¨Ø±Ø§ÛŒ ÙˆØ§Ø±Ø¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø§Ø¶Ø§ÛŒ Ø§Ø±Ø² Ù…Ø³Ø§ÙØ±ØªÛŒ Ùˆ ØªØ¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø³Øª Ø§Ø¬Ø±Ø§Ø³Øª. Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ù…Ø¹ØªÙ‚Ø¯Ù†Ø¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø±Ø§Ù‡Ú©Ø§Ø± Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø§Ø± ØªÙˆØ±Ù… Ùˆ Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø§Ø±Ø²ÛŒØŒ ØªÙ‚ÙˆÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¬Ø°Ø¨ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØµØ§Ø¯Ø±Ø§Øª ØºÛŒØ±Ù†ÙØªÛŒ Ø§Ø³Øª. Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø®Ø´ Ø®ØµÙˆØµÛŒ Ù†ÛŒØ² Ø®ÙˆØ§Ø³ØªØ§Ø± Ú©Ø§Ù‡Ø´ Ù…ÙˆØ§Ù†Ø¹ ØªØ¬Ø§Ø±ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ù†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ù‡ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ Ù…ØªÙ†:  Ø¯Ø± Ù¾ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ùˆ Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø±Ø® Ø¯Ù„Ø§Ø± Ø¯Ø± Ù‡ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ù†Ú© Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø§ Ø§ØªØ®Ø§Ø° Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ØªÙ„Ø§Ø´ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø«Ø¨Ø§Øª Ù†Ø³Ø¨ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø² Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ØŒ Ø¹Ø±Ø¶Ù‡ Ø§Ø±Ø² Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡ Ù†ÛŒÙ…Ø§ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ùˆ Ø³Ù‚Ù Ø®Ø±ÛŒØ¯ Ø§Ø±Ø² Ø¨Ø±Ø§ÛŒ ÙˆØ§Ø±Ø¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø§Ø¶Ø§ÛŒ Ø§Ø±Ø² Ù…Ø³Ø§ÙØ±ØªÛŒ Ùˆ ØªØ¬Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø³Øª Ø§Ø¬Ø±Ø§Ø³Øª. Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ù…Ø¹ØªÙ‚Ø¯Ù†Ø¯ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø±Ø§Ù‡Ú©Ø§Ø± Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø§Ø± ØªÙˆØ±Ù… Ùˆ Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø§Ø±Ø²ÛŒØŒ ØªÙ‚ÙˆÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø¬Ø°Ø¨ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØµØ§Ø¯Ø±Ø§Øª ØºÛŒØ±Ù†ÙØªÛŒ Ø§Ø³Øª. Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø®Ø´ Ø®ØµÙˆØµÛŒ Ù†ÛŒØ² Ø®ÙˆØ§Ø³ØªØ§Ø± Ú©Ø§Ù‡Ø´ Ù…ÙˆØ§Ù†Ø¹ ØªØ¬Ø§Ø±ÛŒ Ùˆ Ø§ØµÙ„Ø§Ø­ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ù†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Ù‡ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: Ø§Ù‚ØªØµØ§Ø¯ÛŒ\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.39%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            ">  Ø¯Ø± Ù…Ø±Ø§Ø³Ù… Ø§ÙØªØªØ§Ø­ÛŒÙ‡ Ø³ÛŒâ€ŒÙˆÙ†Ù‡Ù…ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ ÙÛŒÙ„Ù… ÙØ¬Ø±ØŒ Ú©Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ú¯Ø³ØªØ±Ø¯Ù‡ Ø§Ù‡Ø§Ù„ÛŒ Ø³ÛŒÙ†Ù…Ø§ØŒ Ù…Ù†ØªÙ‚Ø¯Ø§Ù† Ùˆ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¢Ø«Ø§Ø± Ø´Ø§Ø®ØµÛŒ Ø§Ø² Ø³ÛŒÙ†Ù…Ø§Ú¯Ø±Ø§Ù† Ø¬ÙˆØ§Ù† Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø¢Ù…Ø¯. Ø§Ù…Ø³Ø§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø§ÙˆØ±ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ù…Ø³Ø§Ø¨Ù‚Ù‡ØŒ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ú˜Ø§Ù†Ø±Ù‡Ø§ Ùˆ Ø³Ø¨Ú©â€ŒÙ‡Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ù…Ø³Ø§Ø¦Ù„ Ø±ÙˆØ² Ø¬Ø§Ù…Ø¹Ù‡ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨ÛŒØ´ØªØ± Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø­Ø§Ø´ÛŒÙ‡ Ø§ÛŒÙ† Ù…Ø±Ø§Ø³Ù…ØŒ Ø§Ø² Ú†Ù†Ø¯ Ù‡Ù†Ø±Ù…Ù†Ø¯ Ù¾ÛŒØ´Ú©Ø³ÙˆØª Ù†ÛŒØ² ØªÙ‚Ø¯ÛŒØ± Ø¨Ù‡â€ŒØ¹Ù…Ù„ Ø¢Ù…Ø¯ Ùˆ Ù†Ø´Ø³ØªÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Â«Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ù‡ÙˆÛŒØª ÙØ±Ù‡Ù†Ú¯ÛŒÂ» Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯. Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ù‡Ø¯Ù Ø§Ù…Ø³Ø§Ù„ØŒ ØªÙ‚ÙˆÛŒØª ØªØ¹Ø§Ù…Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø³ÛŒÙ†Ù…Ø§Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø­Ù…Ø§ÛŒØª Ø§Ø² ØªÙˆÙ„ÛŒØ¯Ø§Øª Ù…Ø³ØªÙ‚Ù„ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø³Øª.\n",
            "\n",
            "ğŸ“ Ù…ØªÙ†:  Ø¯Ø± Ù…Ø±Ø§Ø³Ù… Ø§ÙØªØªØ§Ø­ÛŒÙ‡ Ø³ÛŒâ€ŒÙˆÙ†Ù‡Ù…ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ ÙÛŒÙ„Ù… ÙØ¬Ø±ØŒ Ú©Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ú¯Ø³ØªØ±Ø¯Ù‡ Ø§Ù‡Ø§Ù„ÛŒ Ø³ÛŒÙ†Ù…Ø§ØŒ Ù…Ù†ØªÙ‚Ø¯Ø§Ù† Ùˆ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¢Ø«Ø§Ø± Ø´Ø§Ø®ØµÛŒ Ø§Ø² Ø³ÛŒÙ†Ù…Ø§Ú¯Ø±Ø§Ù† Ø¬ÙˆØ§Ù† Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø¢Ù…Ø¯. Ø§Ù…Ø³Ø§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø§ÙˆØ±ÛŒ Ùˆ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ Ù…Ø³Ø§Ø¨Ù‚Ù‡ØŒ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ú˜Ø§Ù†Ø±Ù‡Ø§ Ùˆ Ø³Ø¨Ú©â€ŒÙ‡Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø®ØªÙ† Ø¨Ù‡ Ù…Ø³Ø§Ø¦Ù„ Ø±ÙˆØ² Ø¬Ø§Ù…Ø¹Ù‡ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨ÛŒØ´ØªØ± Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø­Ø§Ø´ÛŒÙ‡ Ø§ÛŒÙ† Ù…Ø±Ø§Ø³Ù…ØŒ Ø§Ø² Ú†Ù†Ø¯ Ù‡Ù†Ø±Ù…Ù†Ø¯ Ù¾ÛŒØ´Ú©Ø³ÙˆØª Ù†ÛŒØ² ØªÙ‚Ø¯ÛŒØ± Ø¨Ù‡â€ŒØ¹Ù…Ù„ Ø¢Ù…Ø¯ Ùˆ Ù†Ø´Ø³ØªÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Â«Ø³ÛŒÙ†Ù…Ø§ Ùˆ Ù‡ÙˆÛŒØª ÙØ±Ù‡Ù†Ú¯ÛŒÂ» Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯. Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ù‡Ø¯Ù Ø§Ù…Ø³Ø§Ù„ØŒ ØªÙ‚ÙˆÛŒØª ØªØ¹Ø§Ù…Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ø¨Ø§ Ø³ÛŒÙ†Ù…Ø§Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø­Ù…Ø§ÛŒØª Ø§Ø² ØªÙˆÙ„ÛŒØ¯Ø§Øª Ù…Ø³ØªÙ‚Ù„ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø³Øª.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 96.23%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            ">  ØªÛŒÙ… Ù…Ù„ÛŒ ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ú†Ù‡Ø§Ø±Ù…ÛŒÙ† Ø¯ÛŒØ¯Ø§Ø± Ø®ÙˆØ¯ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ú¯Ø±ÙˆÙ‡ÛŒ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯ Ù…Ù„Øªâ€ŒÙ‡Ø§ÛŒ Û²Û°Û²Ûµ Ø¨Ù‡ Ù…ØµØ§Ù ØªÛŒÙ… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø±ÙØª. Ø§ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„Ù† Ø§Ù„Ù…Ù¾ÛŒÚ© ØªÙˆÚ©ÛŒÙˆ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù¾Ø±Ø´ÙˆØ± ØªÙ…Ø§Ø´Ø§Ú¯Ø±Ø§Ù† Ùˆ Ù¾Ø®Ø´ Ø²Ù†Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø´Ø¨Ú©Ù‡ ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨ÙˆØ¯. ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ùˆ Ø¯ÙØ§Ø¹ Ù…Ù†Ø³Ø¬Ù…ØŒ Ù…ÙˆÙÙ‚ Ø´Ø¯Ù†Ø¯ Ø³Øª Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ Ø¨Ù‡â€ŒÙ†ÙØ¹ Ø®ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ø³Ø§Ù†Ù†Ø¯. Ù‡Ø±Ú†Ù†Ø¯ ØªÛŒÙ… Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø¯Ø± Ø³Øª Ø¯ÙˆÙ… Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¯Ø§Ø´ØªØŒ Ø§Ù…Ø§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ø¯Ø±Ø®Ø´Ø´ Ø³Ø¹ÛŒØ¯ Ù…Ø¹Ø±ÙˆÙ Ùˆ Ø§Ù…ÛŒØ± ØºÙÙˆØ±ØŒ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø¯Ø± Ø³Øª Ù¾Ù†Ø¬Ù… Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ Û³ Ø¨Ø± Û² Ø¨Ù‡ Ø³ÙˆØ¯ Ø®ÙˆØ¯ ØªÙ…Ø§Ù… Ú©Ø±Ø¯. Ø³Ø±Ù…Ø±Ø¨ÛŒ ØªÛŒÙ… Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø§Ø² ØªØ¹Ù‡Ø¯ Ùˆ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†Ø´ ØªÙ…Ø¬ÛŒØ¯ Ú©Ø±Ø¯ Ùˆ ÙˆØ¹Ø¯Ù‡ Ø¯Ø§Ø¯ ØªÛŒÙ… Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ø­Ø§Ø¶Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
            "\n",
            "ğŸ“ Ù…ØªÙ†:  ØªÛŒÙ… Ù…Ù„ÛŒ ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ú†Ù‡Ø§Ø±Ù…ÛŒÙ† Ø¯ÛŒØ¯Ø§Ø± Ø®ÙˆØ¯ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ú¯Ø±ÙˆÙ‡ÛŒ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯ Ù…Ù„Øªâ€ŒÙ‡Ø§ÛŒ Û²Û°Û²Ûµ Ø¨Ù‡ Ù…ØµØ§Ù ØªÛŒÙ… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø±ÙØª. Ø§ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„Ù† Ø§Ù„Ù…Ù¾ÛŒÚ© ØªÙˆÚ©ÛŒÙˆ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ØŒ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù¾Ø±Ø´ÙˆØ± ØªÙ…Ø§Ø´Ø§Ú¯Ø±Ø§Ù† Ùˆ Ù¾Ø®Ø´ Ø²Ù†Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø´Ø¨Ú©Ù‡ ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨ÙˆØ¯. ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ùˆ Ø¯ÙØ§Ø¹ Ù…Ù†Ø³Ø¬Ù…ØŒ Ù…ÙˆÙÙ‚ Ø´Ø¯Ù†Ø¯ Ø³Øª Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø§Ù„Ø§ Ø¨Ù‡â€ŒÙ†ÙØ¹ Ø®ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ø³Ø§Ù†Ù†Ø¯. Ù‡Ø±Ú†Ù†Ø¯ ØªÛŒÙ… Ø§ÛŒØªØ§Ù„ÛŒØ§ Ø¯Ø± Ø³Øª Ø¯ÙˆÙ… Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¯Ø§Ø´ØªØŒ Ø§Ù…Ø§ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§ÛŒØ±Ø§Ù† Ø¨Ø§ Ø¯Ø±Ø®Ø´Ø´ Ø³Ø¹ÛŒØ¯ Ù…Ø¹Ø±ÙˆÙ Ùˆ Ø§Ù…ÛŒØ± ØºÙÙˆØ±ØŒ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø¯Ø± Ø³Øª Ù¾Ù†Ø¬Ù… Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡ Û³ Ø¨Ø± Û² Ø¨Ù‡ Ø³ÙˆØ¯ Ø®ÙˆØ¯ ØªÙ…Ø§Ù… Ú©Ø±Ø¯. Ø³Ø±Ù…Ø±Ø¨ÛŒ ØªÛŒÙ… Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø§Ø² ØªØ¹Ù‡Ø¯ Ùˆ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†Ø´ ØªÙ…Ø¬ÛŒØ¯ Ú©Ø±Ø¯ Ùˆ ÙˆØ¹Ø¯Ù‡ Ø¯Ø§Ø¯ ØªÛŒÙ… Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø±Ù‚Ø§Ø¨Øªâ€ŒÙ‡Ø§ Ø­Ø§Ø¶Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ÙˆØ±Ø²Ø´ÛŒ\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.96%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            ">  Ø¯Ø± Ø¢Ø³ØªØ§Ù†Ù‡ ÙØµÙ„ ØªØ§Ø¨Ø³ØªØ§Ù†ØŒ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡â€ŒÚ©Ø§Ø± Ú©Ø±Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø³ØªØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø´ÙˆØ± Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ ØºØ±ÙÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø¬Ø§Ø°Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒØŒ Ø¢ÛŒÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒØŒ ØºØ°Ø§Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯Ø§Øª ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ø±Ø¯Ù†Ø¯. ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ù†ÙˆØ§Ø­ÛŒØŒ Ú©Ø§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø³ÙØ§Ù„â€ŒÚ¯Ø±ÛŒØŒ ÙØ±Ø´â€ŒØ¨Ø§ÙÛŒ Ùˆ Ø³Ø§Ø®Øª Ø²ÛŒÙˆØ±Ø¢Ù„Ø§Øª Ø³Ù†ØªÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ ØªÙˆØ§Ù†Ø³Øª ØªÙˆØ¬Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ø¬Ù„Ø¨ Ú©Ù†Ø¯. Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ ØªØ±ÙˆÛŒØ¬ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø´Ø§ØºÙ„ Ø¨ÙˆÙ…ÛŒ Ùˆ ØªÙ‚ÙˆÛŒØª Ø§Ù‚ØªØµØ§Ø¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ± ØªØ®ÙÛŒÙÛŒ Ù†ÛŒØ² ØªÙˆØ³Ø· Ø¢Ú˜Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø³Ø§ÙØ±ØªÛŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.\n",
            "\n",
            "ğŸ“ Ù…ØªÙ†:  Ø¯Ø± Ø¢Ø³ØªØ§Ù†Ù‡ ÙØµÙ„ ØªØ§Ø¨Ø³ØªØ§Ù†ØŒ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¢ØºØ§Ø² Ø¨Ù‡â€ŒÚ©Ø§Ø± Ú©Ø±Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø³ØªØ§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø´ÙˆØ± Ø¨Ø§ Ø§Ø±Ø§Ø¦Ù‡ ØºØ±ÙÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø¬Ø§Ø°Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒØŒ Ø¢ÛŒÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒØŒ ØºØ°Ø§Ù‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ Ùˆ ØªÙˆÙ„ÛŒØ¯Ø§Øª ØµÙ†Ø§ÛŒØ¹â€ŒØ¯Ø³ØªÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ø±Ø¯Ù†Ø¯. ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù†Ø¯Ù‡ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ù†ÙˆØ§Ø­ÛŒØŒ Ú©Ø§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø³ÙØ§Ù„â€ŒÚ¯Ø±ÛŒØŒ ÙØ±Ø´â€ŒØ¨Ø§ÙÛŒ Ùˆ Ø³Ø§Ø®Øª Ø²ÛŒÙˆØ±Ø¢Ù„Ø§Øª Ø³Ù†ØªÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ ØªÙˆØ§Ù†Ø³Øª ØªÙˆØ¬Ù‡ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ø¬Ù„Ø¨ Ú©Ù†Ø¯. Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø±Ú©Ù†Ù†Ø¯Ù‡ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø±Ø§ ØªØ±ÙˆÛŒØ¬ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒØŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø´Ø§ØºÙ„ Ø¨ÙˆÙ…ÛŒ Ùˆ ØªÙ‚ÙˆÛŒØª Ø§Ù‚ØªØµØ§Ø¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø³ÙØ± ØªØ®ÙÛŒÙÛŒ Ù†ÛŒØ² ØªÙˆØ³Ø· Ø¢Ú˜Ø§Ù†Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø³Ø§ÙØ±ØªÛŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.63%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            "> exit\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/Final1/final-bert-fa-lora/best_model\"\n",
        "\n",
        "# ===== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± =====\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "label_names = [\"Ø§Ù‚ØªØµØ§Ø¯ÛŒ\", \"ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·\", \"ÙØ±Ù‡Ù†Ú¯\", \"ÙˆØ±Ø²Ø´ÛŒ\"]\n",
        "\n",
        "# ===== ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ =====\n",
        "def classify_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        pred_class = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_class].item()\n",
        "\n",
        "    print(f\"\\nğŸ“ Ù…ØªÙ†: {text}\")\n",
        "    print(f\"ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: {label_names[pred_class]}\")\n",
        "    print(f\"ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: {confidence:.2%}\")\n",
        "\n",
        "# ===== ØªØ³Øª Ø¨Ø§ ÛŒÚ© ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡ =====\n",
        "while True:\n",
        "    user_input = input(\"\\nğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\\n> \")\n",
        "    if user_input.strip().lower() == \"exit\":\n",
        "        break\n",
        "    classify_text(user_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "0HbDREDDPl2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Final1/final-bert-fa-lora/best_model\"\n",
        "\n",
        "# ===== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± =====\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "label_names = [\"Ø§Ù‚ØªØµØ§Ø¯ÛŒ\", \"ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·\", \"ÙØ±Ù‡Ù†Ú¯\", \"ÙˆØ±Ø²Ø´ÛŒ\"]\n",
        "\n",
        "# ===== ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ =====\n",
        "def classify_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        pred_class = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_class].item()\n",
        "\n",
        "    print(f\"\\nğŸ“ Ù…ØªÙ†: {text}\")\n",
        "    print(f\"ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: {label_names[pred_class]}\")\n",
        "    print(f\"ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: {confidence:.2%}\")\n",
        "\n",
        "# ===== ØªØ³Øª Ø¨Ø§ ÛŒÚ© ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡ =====\n",
        "while True:\n",
        "    user_input = input(\"\\nğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\\n> \")\n",
        "    if user_input.strip().lower() == \"exit\":\n",
        "        break\n",
        "    classify_text(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAnZMzwUPmlo",
        "outputId": "7fff6cbc-e2f9-4176-d6bf-83214f047e1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading file chat_template.jinja\n",
            "loading configuration file /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.3,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\",\n",
            "    \"1\": \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\",\n",
            "    \"2\": \"\\u0641\\u0631\\u0647\\u0646\\u06af\",\n",
            "    \"3\": \"\\u0648\\u0631\\u0632\\u0634\\u06cc\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"\\u0627\\u0642\\u062a\\u0635\\u0627\\u062f\\u06cc\": 0,\n",
            "    \"\\u062a\\u0641\\u0631\\u06cc\\u062d \\u0648 \\u0646\\u0634\\u0627\\u0637\": 1,\n",
            "    \"\\u0641\\u0631\\u0647\\u0646\\u06af\": 2,\n",
            "    \"\\u0648\\u0631\\u0632\\u0634\\u06cc\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.53.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model/model.safetensors\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Final1/final-bert-fa-lora/best_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            "> Ù†Ø³Ø§Ø¬ÛŒ Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† Ø¯Ø± ÙØµÙ„ Ø¬Ø¯ÛŒØ¯ Ù„ÛŒÚ¯ Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ ØªÛŒÙ…ÛŒ Ù¾Ø±Ù…Ù‡Ø±Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±ÛŒØ¹ Ø¨Ù‡ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø±Ú†Ù‡ Ù†Ø³Ø§Ø¬ÛŒ Ù‡Ù†ÙˆØ² Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø±Ø³Ù…ÛŒ ÙØ±Ø§Ø² Ú©Ù…Ø§Ù„ÙˆÙ†Ø¯ Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† Ø³Ø±Ù…Ø±Ø¨ÛŒ Ø§Ø¹Ù„Ø§Ù… Ù†Ú©Ø±Ø¯Ù‡ØŒ Ø§Ù…Ø§ Ù…Ø¯ÛŒØ±Ø§Ù† Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø¨Ø§ Ø§Ùˆ Ø¨Ù‡ ØªÙˆØ§ÙÙ‚ Ù†Ø²Ø¯ÛŒÚ© Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù…Ø°Ø§Ú©Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø°Ø¨ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù† Ø³Ø±Ø´Ù†Ø§Ø³ Ø±Ø§ Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø±ÙˆØ²Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ø´Ú¯Ø§Ù‡ ØªÙ…Ø±ÛŒÙ†Ø§Øª Ø±Ø§ Ø¯Ø± Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ú©Ø±Ø¯Ù‡ Ùˆ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ‚ÙˆÛŒØª ØªÛŒÙ… Ø¨Ø§ Ø¬Ø°Ø¨ Ø³ØªØ§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯ÛŒØ±Ø§Ù† Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ù‚ØµØ¯ Ø¯Ø§Ø±Ù†Ø¯ ØªÛŒÙ…ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ùˆ Ù¾Ø±Ù…Ù‡Ø±Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ ØªØ§ Ù†Ù‡â€ŒØªÙ†Ù‡Ø§ Ø±Ø¶Ø§ÛŒØª Ù‡ÙˆØ§Ø¯Ø§Ø±Ø§Ù† Ø±Ø§ Ø¬Ù„Ø¨ Ú©Ù†Ù†Ø¯ØŒâ€Œ Ø¨Ù„Ú©Ù‡ Ø¨Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ† Ø¨ÛŒÙ† Ø¯Ùˆ ØªÛŒÙ… Ø¨Ø±ØªØ± Ù„ÛŒÚ¯â€ŒÛŒÚ©ØŒ Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± Ø¨Ø§Ø²Ú¯Ø±Ø¯Ù†Ø¯. Ø·Ø¨Ù‚ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ØŒ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡â€ŒØ§ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø´Ø§Ù‡Ø¯ Ø±ÙˆÙ†Ù…Ø§ÛŒÛŒ Ø±Ø³Ù…ÛŒ Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø¨Ø§Ø²ÛŒÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¨ÙˆØ¯.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ Ù…ØªÙ†: Ù†Ø³Ø§Ø¬ÛŒ Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† Ø¯Ø± ÙØµÙ„ Ø¬Ø¯ÛŒØ¯ Ù„ÛŒÚ¯ Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ ØªÛŒÙ…ÛŒ Ù¾Ø±Ù…Ù‡Ø±Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø³Ø±ÛŒØ¹ Ø¨Ù‡ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø±Ú†Ù‡ Ù†Ø³Ø§Ø¬ÛŒ Ù‡Ù†ÙˆØ² Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø±Ø³Ù…ÛŒ ÙØ±Ø§Ø² Ú©Ù…Ø§Ù„ÙˆÙ†Ø¯ Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† Ø³Ø±Ù…Ø±Ø¨ÛŒ Ø§Ø¹Ù„Ø§Ù… Ù†Ú©Ø±Ø¯Ù‡ØŒ Ø§Ù…Ø§ Ù…Ø¯ÛŒØ±Ø§Ù† Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø¨Ø§ Ø§Ùˆ Ø¨Ù‡ ØªÙˆØ§ÙÙ‚ Ù†Ø²Ø¯ÛŒÚ© Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù…Ø°Ø§Ú©Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø°Ø¨ Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù† Ø³Ø±Ø´Ù†Ø§Ø³ Ø±Ø§ Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø±ÙˆØ²Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ±ØŒ Ø¨Ø§Ø´Ú¯Ø§Ù‡ ØªÙ…Ø±ÛŒÙ†Ø§Øª Ø±Ø§ Ø¯Ø± Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ú©Ø±Ø¯Ù‡ Ùˆ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ‚ÙˆÛŒØª ØªÛŒÙ… Ø¨Ø§ Ø¬Ø°Ø¨ Ø³ØªØ§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± Ø¢ØºØ§Ø² Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ù…Ø¯ÛŒØ±Ø§Ù† Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ù‚ØµØ¯ Ø¯Ø§Ø±Ù†Ø¯ ØªÛŒÙ…ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ùˆ Ù¾Ø±Ù…Ù‡Ø±Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ ØªØ§ Ù†Ù‡â€ŒØªÙ†Ù‡Ø§ Ø±Ø¶Ø§ÛŒØª Ù‡ÙˆØ§Ø¯Ø§Ø±Ø§Ù† Ø±Ø§ Ø¬Ù„Ø¨ Ú©Ù†Ù†Ø¯ØŒâ€Œ Ø¨Ù„Ú©Ù‡ Ø¨Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ† Ø¨ÛŒÙ† Ø¯Ùˆ ØªÛŒÙ… Ø¨Ø±ØªØ± Ù„ÛŒÚ¯â€ŒÛŒÚ©ØŒ Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ Ù„ÛŒÚ¯â€ŒØ¨Ø±ØªØ± Ø¨Ø§Ø²Ú¯Ø±Ø¯Ù†Ø¯. Ø·Ø¨Ù‚ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ØŒ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡â€ŒØ§ÛŒ Ù†Ø²Ø¯ÛŒÚ© Ø´Ø§Ù‡Ø¯ Ø±ÙˆÙ†Ù…Ø§ÛŒÛŒ Ø±Ø³Ù…ÛŒ Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø¨Ø§Ø²ÛŒÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¨ÙˆØ¯.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ÙˆØ±Ø²Ø´ÛŒ\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.94%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            "> Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ø²Ø§Ø±Ø´ Ø±Ø³Ù…ÛŒ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ØªØ³Ù†ÛŒÙ…ØŒ Ø¯Ø± Ø®Ø±Ø¯Ø§Ø¯ Ù…Ø§Ù‡ Û±Û´Û°Û´ Ù…ÛŒØ²Ø§Ù† Ø¹Ø±Ø¶Ù‡ Ú¯ÙˆØ´Øª Ù‚Ø±Ù…Ø² Ø¯Ø± Ú©Ø´ØªØ§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ± Ø­Ø¯ÙˆØ¯ Û±Ûµ Ø¯Ø±ØµØ¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§Ø±Ø¯ÛŒØ¨Ù‡Ø´Øª Ù…Ø§Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ø±Ø¶Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø´Ø§Ø®Øµ Ù…Ø«Ø¨Øª Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø§Ù‚ØªØµØ§Ø¯ Ù…ÙˆØ§Ø¯ ØºØ°Ø§ÛŒÛŒ Ú©Ø´ÙˆØ± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. ÙˆØ²Ø§Ø±Øª Ø¬Ù‡Ø§Ø¯ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ø§ÛŒÙ† Ø±Ø´Ø¯ Ù†Ø§Ø´ÛŒ Ø§Ø² ØªÙ‚ÙˆÛŒØª ÙˆØ§Ø±Ø¯Ø§Øª Ø¯Ø§Ù…ØŒ Ø¨Ù‡â€ŒÚ©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¸Ø±ÙÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø´ØªØ§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ´ÙˆÛŒÙ‚ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù…Ø¯Ø§Ø±Ø§Ù† Ø¯Ø± Ø¬Ù‡Øª Ø¹Ø±Ø¶Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… ØªÙˆÙ„ÛŒØ¯Ø§Øª Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ø±Ø¶Ù‡ Ø±Ø³Ù…ÛŒØŒ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ ÙØ´Ø§Ø± Ù‚ÛŒÙ…ØªÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯ Ùˆ Ø§Ø² Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ø±ÙˆÙ†Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± Ú©Ù†ØªØ±Ù„ ØªÙˆØ±Ù… ØºØ°Ø§ÛŒÛŒ Ø§ÛŒÙØ§ Ú©Ù†Ø¯ØŒ Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù…Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ù… Ø³Ø§Ù„ Ú©Ù‡ Ù…ØµØ±Ù Ø®Ø§Ù†ÙˆØ§Ø±Ù‡Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯. Ø§Ø² Ø³ÙˆÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‚Ø§Ú†Ø§Ù‚ Ø¯Ø§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ú©Ù„Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¯ÙˆÙ„Øª Ù…Ø·Ø±Ø­ Ø´Ø¯Ù‡ Ùˆ Ø¹Ø±Ø¶Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ ØªØ­Ù‚Ù‚ Ø§ÛŒÙ† Ù‡Ø¯Ù Ù†ÛŒØ² Ú©Ù…Ú© Ú©Ù†Ø¯. Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ØŒ Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±Ø§Ù† Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù†Ø¯ Ø§ÛŒÙ† Ø§ÙØ²Ø§ÛŒØ´ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ø§Ø´Ø¯ Ùˆ Ø¨Ù‡ Ø§ÛŒØ¬Ø§Ø¯ ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† ØªÙˆÙ„ÛŒØ¯ØŒ Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§ Ø¯Ø± Ø²Ù†Ø¬ÛŒØ±Ù‡ ØªØ§Ù…ÛŒÙ† Ú¯ÙˆØ´Øª Ú©Ø´ÙˆØ± Ù…Ù†Ø¬Ø± Ø´ÙˆØ¯.\n",
            "\n",
            "ğŸ“ Ù…ØªÙ†: Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ø²Ø§Ø±Ø´ Ø±Ø³Ù…ÛŒ Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ ØªØ³Ù†ÛŒÙ…ØŒ Ø¯Ø± Ø®Ø±Ø¯Ø§Ø¯ Ù…Ø§Ù‡ Û±Û´Û°Û´ Ù…ÛŒØ²Ø§Ù† Ø¹Ø±Ø¶Ù‡ Ú¯ÙˆØ´Øª Ù‚Ø±Ù…Ø² Ø¯Ø± Ú©Ø´ØªØ§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ± Ø­Ø¯ÙˆØ¯ Û±Ûµ Ø¯Ø±ØµØ¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§Ø±Ø¯ÛŒØ¨Ù‡Ø´Øª Ù…Ø§Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ø±Ø¶Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø´Ø§Ø®Øµ Ù…Ø«Ø¨Øª Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø§Ù‚ØªØµØ§Ø¯ Ù…ÙˆØ§Ø¯ ØºØ°Ø§ÛŒÛŒ Ú©Ø´ÙˆØ± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. ÙˆØ²Ø§Ø±Øª Ø¬Ù‡Ø§Ø¯ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ø§ÛŒÙ† Ø±Ø´Ø¯ Ù†Ø§Ø´ÛŒ Ø§Ø² ØªÙ‚ÙˆÛŒØª ÙˆØ§Ø±Ø¯Ø§Øª Ø¯Ø§Ù…ØŒ Ø¨Ù‡â€ŒÚ©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¸Ø±ÙÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø´ØªØ§Ø±Ú¯Ø§Ù‡â€ŒÙ‡Ø§ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ´ÙˆÛŒÙ‚ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù…Ø¯Ø§Ø±Ø§Ù† Ø¯Ø± Ø¬Ù‡Øª Ø¹Ø±Ø¶Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… ØªÙˆÙ„ÛŒØ¯Ø§Øª Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¹Ø±Ø¶Ù‡ Ø±Ø³Ù…ÛŒØŒ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ ÙØ´Ø§Ø± Ù‚ÛŒÙ…ØªÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯ Ùˆ Ø§Ø² Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø´Ø¯ÛŒØ¯ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ø±ÙˆÙ†Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± Ú©Ù†ØªØ±Ù„ ØªÙˆØ±Ù… ØºØ°Ø§ÛŒÛŒ Ø§ÛŒÙØ§ Ú©Ù†Ø¯ØŒ Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ù…Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ù… Ø³Ø§Ù„ Ú©Ù‡ Ù…ØµØ±Ù Ø®Ø§Ù†ÙˆØ§Ø±Ù‡Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯. Ø§Ø² Ø³ÙˆÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‚Ø§Ú†Ø§Ù‚ Ø¯Ø§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ú©Ù„Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¯ÙˆÙ„Øª Ù…Ø·Ø±Ø­ Ø´Ø¯Ù‡ Ùˆ Ø¹Ø±Ø¶Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ ØªØ­Ù‚Ù‚ Ø§ÛŒÙ† Ù‡Ø¯Ù Ù†ÛŒØ² Ú©Ù…Ú© Ú©Ù†Ø¯. Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ØŒ Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±Ø§Ù† Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù†Ø¯ Ø§ÛŒÙ† Ø§ÙØ²Ø§ÛŒØ´ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ø§Ø´Ø¯ Ùˆ Ø¨Ù‡ Ø§ÛŒØ¬Ø§Ø¯ ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† ØªÙˆÙ„ÛŒØ¯ØŒ Ø¹Ø±Ø¶Ù‡ Ùˆ ØªÙ‚Ø§Ø¶Ø§ Ø¯Ø± Ø²Ù†Ø¬ÛŒØ±Ù‡ ØªØ§Ù…ÛŒÙ† Ú¯ÙˆØ´Øª Ú©Ø´ÙˆØ± Ù…Ù†Ø¬Ø± Ø´ÙˆØ¯.\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: Ø§Ù‚ØªØµØ§Ø¯ÛŒ\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.85%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            "> Ø¯Ø± Ø§Ø³ØªØ§Ù† Ù‚Ø²ÙˆÛŒÙ† Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆÛŒ Ø¨Ø²Ø±Ú¯ Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒØŒ Ø¬Ù…Ø¹ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² Ø´Ù‡Ø±ÙˆÙ†Ø¯Ø§Ù† Ø¯Ø± Ù…ÛŒØ§Ù† Ú©ÙˆÙ‡â€ŒÙ‡Ø§ Ùˆ Ø·Ø¨ÛŒØ¹Øª Ù…Ù†Ø·Ù‚Ù‡ Ø­Ø§Ø¶Ø± Ø´Ø¯Ù†Ø¯ ØªØ§ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± ÙˆØ±Ø²Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ØŒ ÙØ±Ù‡Ù†Ú¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø±Ø§ Ù†ÛŒØ² ØªÙ‚ÙˆÛŒØª Ú©Ù†Ù†Ø¯. Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ø¨Ø§ Ù‡Ø¯Ù Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø·Ø­ Ø³Ù„Ø§Ù…Øª Ø¬Ø³Ù…Ø§Ù†ÛŒ Ùˆ Ø±ÙˆØ­ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.  Ø¨Ø± Ø®Ù„Ø§Ù Ø¯ÛŒØ¯Ú¯Ø§Ù‡ Ø³Ù†ØªÛŒØŒ Ø§ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù‚Ø´Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¬Ø§Ù…Ø¹Ù‡ â€“ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ú©ÙˆØ¯Ú©Ø§Ù†ØŒ Ù†ÙˆØ¬ÙˆØ§Ù†Ø§Ù†ØŒ ÙˆØ§Ù„Ø¯ÛŒÙ† Ùˆ Ø­ØªÛŒ Ø³Ø§Ù„Ù…Ù†Ø¯Ø§Ù† â€“ ØªÙˆØ§Ù†Ø³Øª ÙØ¶Ø§ÛŒÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯ Ú©Ù‡ Ø§ÙØ±Ø§Ø¯ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø·Ø¨ÛŒØ¹Øªâ€ŒÚ¯Ø±Ø¯ÛŒØŒ ÙØ±ØµØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ùˆ ØªØ¨Ø§Ø¯Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ù†ÛŒØ² Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù†Ø¯. Ù…Ø¯ÛŒØ±Ø§Ù† Ø´Ù‡Ø±ÛŒ Ù‚Ø²ÙˆÛŒÙ† ØªØ£Ú©ÛŒØ¯ Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ± Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒØŒ Ø¨Ø³ØªØ±ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ø±ÙÛŒ Ø¸Ø±ÙÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ±Ù‡Ù†Ú¯ÛŒ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø³ØªØ§Ù† Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.  Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆÛŒØŒ Ø§Ø² Ø¯Ùˆ ÙˆØ±Ø²Ø´Ú©Ø§Ø± Ù¾Ø§Ø±Ø§Ú©Ø§Ø±Ø§ØªÙ‡â€ŒÚ©Ø§Ø± Ù…Ù†ØªØ®Ø¨ Ù‚Ø²ÙˆÛŒÙ†ØŒ Ù„ÛŒÙ„Ø§ Ú†Ø§Ù„ÛŒØ§Ù† Ùˆ Ù…Ø­Ù…Ø¯ Ø¬Ø¹ÙØ±ÛŒØŒ ØªÙ‚Ø¯ÛŒØ± Ø´Ø¯ ØªØ§ Ø¶Ù…Ù† Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø¯Ù† Ø¨Ù‡ ÙˆØ±Ø²Ø´ Ù…Ø¹Ù„ÙˆÙ„Ø§Ù†ØŒ Ø¬Ù„ÙˆÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø´Ø§Ø· ØªÛŒÙ…ÛŒ Ù†ÛŒØ² Ø¯Ø± Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¯ÛŒØ¯Ù‡ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨Ù‡ ÛŒÚ© Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¬Ø§Ù…Ø¹ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ ÙˆØ±Ø²Ø´ØŒ ØªÙØ±ÛŒØ­ØŒ ÙØ±Ù‡Ù†Ú¯ Ùˆ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´ÙˆØ¯\n",
            "\n",
            "ğŸ“ Ù…ØªÙ†: Ø¯Ø± Ø§Ø³ØªØ§Ù† Ù‚Ø²ÙˆÛŒÙ† Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆÛŒ Ø¨Ø²Ø±Ú¯ Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒØŒ Ø¬Ù…Ø¹ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² Ø´Ù‡Ø±ÙˆÙ†Ø¯Ø§Ù† Ø¯Ø± Ù…ÛŒØ§Ù† Ú©ÙˆÙ‡â€ŒÙ‡Ø§ Ùˆ Ø·Ø¨ÛŒØ¹Øª Ù…Ù†Ø·Ù‚Ù‡ Ø­Ø§Ø¶Ø± Ø´Ø¯Ù†Ø¯ ØªØ§ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± ÙˆØ±Ø²Ø´ Ø±ÙˆØ²Ø§Ù†Ù‡ØŒ ÙØ±Ù‡Ù†Ú¯ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ùˆ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø±Ø§ Ù†ÛŒØ² ØªÙ‚ÙˆÛŒØª Ú©Ù†Ù†Ø¯. Ø§ÛŒÙ† Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ø¨Ø§ Ù‡Ø¯Ù Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø·Ø­ Ø³Ù„Ø§Ù…Øª Ø¬Ø³Ù…Ø§Ù†ÛŒ Ùˆ Ø±ÙˆØ­ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.  Ø¨Ø± Ø®Ù„Ø§Ù Ø¯ÛŒØ¯Ú¯Ø§Ù‡ Ø³Ù†ØªÛŒØŒ Ø§ÛŒÙ† Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨Ø§ Ø­Ø¶ÙˆØ± Ù‚Ø´Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¬Ø§Ù…Ø¹Ù‡ â€“ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ú©ÙˆØ¯Ú©Ø§Ù†ØŒ Ù†ÙˆØ¬ÙˆØ§Ù†Ø§Ù†ØŒ ÙˆØ§Ù„Ø¯ÛŒÙ† Ùˆ Ø­ØªÛŒ Ø³Ø§Ù„Ù…Ù†Ø¯Ø§Ù† â€“ ØªÙˆØ§Ù†Ø³Øª ÙØ¶Ø§ÛŒÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯ Ú©Ù‡ Ø§ÙØ±Ø§Ø¯ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø·Ø¨ÛŒØ¹Øªâ€ŒÚ¯Ø±Ø¯ÛŒØŒ ÙØ±ØµØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ùˆ ØªØ¨Ø§Ø¯Ù„ ÙØ±Ù‡Ù†Ú¯ÛŒ Ù†ÛŒØ² Ù¾ÛŒØ¯Ø§ Ú©Ù†Ù†Ø¯. Ù…Ø¯ÛŒØ±Ø§Ù† Ø´Ù‡Ø±ÛŒ Ù‚Ø²ÙˆÛŒÙ† ØªØ£Ú©ÛŒØ¯ Ú©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ± Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒØŒ Ø¨Ø³ØªØ±ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ø±ÙÛŒ Ø¸Ø±ÙÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ±Ù‡Ù†Ú¯ÛŒ Ùˆ Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ Ø§Ø³ØªØ§Ù† Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.  Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ±ÙˆÛŒØŒ Ø§Ø² Ø¯Ùˆ ÙˆØ±Ø²Ø´Ú©Ø§Ø± Ù¾Ø§Ø±Ø§Ú©Ø§Ø±Ø§ØªÙ‡â€ŒÚ©Ø§Ø± Ù…Ù†ØªØ®Ø¨ Ù‚Ø²ÙˆÛŒÙ†ØŒ Ù„ÛŒÙ„Ø§ Ú†Ø§Ù„ÛŒØ§Ù† Ùˆ Ù…Ø­Ù…Ø¯ Ø¬Ø¹ÙØ±ÛŒØŒ ØªÙ‚Ø¯ÛŒØ± Ø´Ø¯ ØªØ§ Ø¶Ù…Ù† Ø§Ù‡Ù…ÛŒØª Ø¯Ø§Ø¯Ù† Ø¨Ù‡ ÙˆØ±Ø²Ø´ Ù…Ø¹Ù„ÙˆÙ„Ø§Ù†ØŒ Ø¬Ù„ÙˆÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ø´Ø§Ø· ØªÛŒÙ…ÛŒ Ù†ÛŒØ² Ø¯Ø± Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¯ÛŒØ¯Ù‡ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ø§Ù‚Ø¯Ø§Ù… Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡ Ø¨Ù‡ ÛŒÚ© Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¬Ø§Ù…Ø¹ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ ÙˆØ±Ø²Ø´ØŒ ØªÙØ±ÛŒØ­ØŒ ÙØ±Ù‡Ù†Ú¯ Ùˆ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´ÙˆØ¯\n",
            "ğŸ“š Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡: ØªÙØ±ÛŒØ­ Ùˆ Ù†Ø´Ø§Ø·\n",
            "ğŸ“Š Ø§Ø¹ØªÙ…Ø§Ø¯ Ù…Ø¯Ù„: 99.18%\n",
            "\n",
            "ğŸ”¹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù† (ÛŒØ§ 'exit' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬):\n",
            "> exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpJQRoFNPob1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}